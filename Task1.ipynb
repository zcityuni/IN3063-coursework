{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN3063 Group 2 Coursework Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Dataset Selection and description](#a-dataset-selection-and-description)\n",
    "2. [Implement sigmoid and ReLU layers](#b-implement-sigmoid-and-relu-layers)\n",
    "3. [Implement dropout](#d-implement-dropout)\n",
    "4. [Implement a fully parameterizable neural network class](#e-implement-a-fully-parametrizable-neural-network-class)\n",
    "5. [Implement optimizer](#f-implement-optimizer)\n",
    "6. [Evaluate different neural network architectures/parameters present and discuss your results](#g-evaluate-different-neural-network-architecturesparameters-present-and-discuss-your-results)\n",
    "7. [Code quality and report and presentation](#h-code-quality-and-report-presentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Dataset selection and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "\n",
    "image, label = mnist_dataset[0]\n",
    "print(\"Label:\" , label)\n",
    "\n",
    "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "plt.axis(\"off\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Implement sigmoid and ReLU layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SigmoidLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward_pass(self, x):\n",
    "        self.activation_output = 1 / (1 + np.exp(-x))\n",
    "        return self.activation_output\n",
    "    \n",
    "    def backward_pass(self, activation):\n",
    "        return (1 - activation) * activation\n",
    "    \n",
    "\n",
    "class ReLULayer:\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        self.output = np.maximum(0, x) #relu is like _/ so max of zero and x \n",
    "        return self.output\n",
    "\n",
    "    def backward_pass(self, upstream_gradient):\n",
    "        return upstream_gradient * np.where(self.output > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Implement softmax layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the softmax layer is to convert raw scores (logits) from the neural network into probabilites that sum to 1.\n",
    "\n",
    "Here is the softmax formula: $ f(z)_i = \\frac{(e^z)_i}{\\Sigma_j^K(e^z)_j} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "    \n",
    "class SoftmaxLayer:\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "    \n",
    "    def forward_pass(self, inputs):\n",
    "        #make sure the dimensions are correct shape\n",
    "        if inputs.ndim == 1:\n",
    "            inputs = inputs.reshape(-1, 1)\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=0, keepdims=True)) #subtract max for stability\n",
    "        #normalise\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=0, keepdims=True)\n",
    "        \n",
    "        self.output = probabilities\n",
    "        return self.output\n",
    "    \n",
    "    def backward_pass(self, activation):\n",
    "        #cross entropy loss\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Implement dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the dropout layer is to improve the model's generalization and reduce overfitting by randomly temporarily disabling a fraction of the neurons during training, to prevent the model over-relying on specific neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DropoutLayer:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "        self.mask = None\n",
    "\n",
    "    def forward_pass(self, inputs, training=True):\n",
    "        if training:\n",
    "            self.mask = (np.random.rand(*inputs.shape) > self.rate) / (1 - self.rate)\n",
    "            return inputs * self.mask\n",
    "        else:\n",
    "            return inputs\n",
    "    \n",
    "    def backward_pass(self, dvalues):\n",
    "        return dvalues * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Implement a fully parametrizable neural network class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedforward neural network\n",
    "You should implement a fully-connected NN class where with number of hidden\n",
    "layers, units, activation functions can be changed. In addition, you can add dropout or\n",
    "regularizer (L1 or L2). \n",
    "\n",
    "\n",
    "Report the parameters used (update rule, learning rate, decay,\n",
    "epochs, batch size) and include the plots in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "class NeuraNetwork:\n",
    "    \n",
    "     def __init__(self, in_nodes, out_nodes, hidden_layers, hidden_activation, output_activation, dropoutRate=0.0, learning_rate=0.001, regularization=None, lambda_=0.01, epochs=100, optimizer='bgd'):\n",
    "        # Intialises the neural network \n",
    "        #Parameters\n",
    "        # inNodes: number of input nodes\n",
    "        # outNodes: number of output nodes\n",
    "        # hidden_layers: list of number of neurons in each hidden layer. eg [128, 64, 32]\n",
    "        # hidden_activation: activation function (uses the forward_pass and backward_pass methods) for hidden layers\n",
    "        # output_activation: activation function for output layer \n",
    "        # dropoutRate: dropout rate to reduce overfitting\n",
    "        # learning_rate: learning rate for training\n",
    "        # regularization: type of regularization l1 or l2\n",
    "        # lambda_: regularization strength\n",
    "        # epochs : the number of epochs that will run over the set\n",
    "        # optimizer : the selected optimizer out of bdg, sgd, minibgd, momentum\n",
    "    \n",
    "        self.in_nodes = in_nodes\n",
    "        self.out_nodes = out_nodes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.output_activation = output_activation\n",
    "        self.dropoutRate = dropoutRate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "        self.lambda_ = lambda_\n",
    "        self.epochs = epochs\n",
    "        self.optimizer = optimizer\n",
    "        self.create_weight_matrices()\n",
    "        self.dropout = DropoutLayer(self.dropoutRate)\n",
    "\n",
    "     def truncated_normal(self, mean=0, sd=1, low=0, upp=10):\n",
    "        return truncnorm(\n",
    "            (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
    "        #Truncated distribution with a specified mean, standard diviation, and bounds\n",
    "\n",
    "     def create_weight_matrices(self):\n",
    "        \"\"\"method to initialize the weight matrices of the neural network\"\"\"\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        #init input to first hiddn layer - make it random so its not all zeroes\n",
    "        rad = 1 / np.sqrt(self.in_nodes) #type of xavier init - to prevent activations and gradients from growing in a exponential way between layers\n",
    "        X = self.truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
    "        self.weights.append(X.rvs((self.hidden_layers[0], self.in_nodes))) #initial dimensions is num of neurons in first hidden layer by amount of features\n",
    "        self.biases.append(np.zeros((self.hidden_layers[0], 1))) #inital bias col vector \n",
    "\n",
    "        #init hidden layer to next hidden layer for all hidden layers\n",
    "        for i in range(1, len(self.hidden_layers)):\n",
    "            rad = 1 / np.sqrt(self.hidden_layers[i-1])\n",
    "            X = self.truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
    "            self.weights.append(X.rvs((self.hidden_layers[i], self.hidden_layers[i-1])))\n",
    "            self.biases.append(np.zeros((self.hidden_layers[i], 1)))\n",
    "\n",
    "        #init last layer to output layer\n",
    "        rad = 1 / np.sqrt(self.hidden_layers[-1])\n",
    "        X = self.truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
    "        self.weights.append(X.rvs((self.out_nodes, self.hidden_layers[-1])))\n",
    "        self.biases.append(np.zeros((self.out_nodes, 1)))\n",
    "\n",
    "     def apply_regularization(self, weights):\n",
    "        \"\"\"applies L1 or L2 regularization.\"\"\"\n",
    "        if self.regularization == 'L1':\n",
    "            return self.lambda_ * np.sign(weights)\n",
    "        elif self.regularization == 'L2':\n",
    "            return self.lambda_ * weights\n",
    "        return 0\n",
    "\n",
    "     def forward_pass(self, input_vector, training=True):\n",
    "        \"\"\"perform a forward pass, each layer has its own weights and bias matrices as init above\"\"\"\n",
    "        activations = [input_vector] #init the activations list to be the input vector \n",
    "        for i in range(len(self.weights) -1 ): #for each layer in the nn up to output layer\n",
    "            #calculate the z value of the neuron which is the dot product of This Layer's weights and the activations of the previous layer added with bias col vector [Z = W(L) * a(L-1) + b(L)]\n",
    "            z = np.dot(self.weights[i], activations[-1]) + self.biases[i] \n",
    "            a = self.hidden_activation.forward_pass(z) #calc the activation value with forward_pass method of the activation func sigmoid or ReLU or softMax\n",
    "            if training and self.dropoutRate > 0:\n",
    "                a = self.dropout.forward_pass(a, training) #apply dropout mask if training\n",
    "            activations.append(a) #update the list of activations, the final nn output is activations[-1] \n",
    "        \n",
    "        #output layer activations\n",
    "        z_final = np.dot(self.weights[-1], activations[-1]) + self.biases[-1]\n",
    "        a_final = self.output_activation.forward_pass(z_final)\n",
    "        activations.append(a_final)\n",
    "\n",
    "        return activations\n",
    "    \n",
    "\n",
    "     def backpropagation(self, activations, target_vector):\n",
    "        \"\"\"perform backward pass to compute gradients\"\"\"\n",
    "        error = [target_vector - activations[-1]] #error for each layer \n",
    "        gradients_w = []\n",
    "        gradients_b = []\n",
    "\n",
    "        #starting from the final layer back to the first layer \n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            #ok so derivative of output error with respect to weight dL/dW(L) = (a(L) - y) * a(L)(1-a(L)) * a(L-1) (in the case of sigmoid)\n",
    "            #error[-1] contains (a(L) - y)\n",
    "            #so we multiply it by the derivative of the activation function a(L)(1-a(L)) for sigmoid \n",
    "            #so delta = dL/dZ = dL/dA * dA/dZ. \n",
    "            delta = error[-1] * self.hidden_activation.backward_pass(activations[i+1])\n",
    "            #to get the gradient dL/dw(L) we need to multiply dL/dZ (delta) by dZ(L)/dW(L). Which is same as delta * a(L-1) as well as any regularization that prevents overfitting\n",
    "            gradients_w.append(np.dot(delta, activations[i].T) + self.apply_regularization(self.weights[i]))\n",
    "            gradients_b.append(delta) # dL/db(L) = dL/∂Z(L) * dZ(L)/db(L) = delta * 1 = delta\n",
    "            if i > 0:\n",
    "                error.append(np.dot(self.weights[i].T, delta)) #propogate the error backward to get each neurons contribution to error \n",
    "\n",
    "        gradients_w.reverse()\n",
    "        gradients_b.reverse()\n",
    "        return gradients_w, gradients_b\n",
    "\n",
    "     def train(self, input_data, target_data):\n",
    "        \"\"\"\n",
    "        train with basic gradient descent over given number of epoches\n",
    "        \"\"\"\n",
    "        input_data = np.array(input_data)\n",
    "        target_data = np.array(target_data)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            total_loss = 0\n",
    "            correct_predictions = 0\n",
    "\n",
    "            for sample in range(len(input_data)): #for each sample we do a full forward pass and backward pass and update of the weights\n",
    "                \n",
    "                #make sure theyre in the correct dimensions\n",
    "                input_vector = np.expand_dims(input_data[sample], axis=1)\n",
    "                target_vector = np.expand_dims(target_data[sample], axis=1)\n",
    "                \n",
    "                activations = self.forward_pass(input_vector, training=True)\n",
    "\n",
    "                #calc cross-entropy loss\n",
    "                epsilon = 1e-15  #prevent log(0)\n",
    "                output = np.clip(activations[-1], epsilon, 1 - epsilon)\n",
    "                loss = -np.sum(target_vector * np.log(output))\n",
    "                total_loss += loss\n",
    "                \n",
    "                #track the accuracy\n",
    "                predicted = np.argmax(activations[-1])\n",
    "                true_label = np.argmax(target_vector)\n",
    "                if predicted == true_label:\n",
    "                    correct_predictions += 1\n",
    "\n",
    "                gradients_w, gradients_b = self.backpropagation(activations, target_vector)\n",
    "\n",
    "                #update each layers weights \n",
    "                for i in range(len(self.weights)):\n",
    "                    self.weights[i] += self.learning_rate * gradients_w[i]\n",
    "                    self.biases[i] += self.learning_rate * np.sum(gradients_b[i], axis=1, keepdims=True)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs} completed.\")\n",
    "            \n",
    "        #print current epoch stats\n",
    "        avg_loss = total_loss / len(input_data)\n",
    "        accuracy = correct_predictions / len(input_data) * 100\n",
    "        print(f\"Epoch {epoch + 1}/{self.epochs} - Loss: {avg_loss:.4f} - Accuracy: {accuracy:.2f}%\")\n",
    "            \n",
    "     def run(self, input_vector):\n",
    "        \"\"\"\n",
    "        prediction method, does a forward pass through the network on the input vector and returns the final activation vector\n",
    "        \"\"\"\n",
    "        input_vector = np.array(input_vector, ndmin=2).T \n",
    "        activations = [input_vector]\n",
    "        \n",
    "        #forward pass through the network (no dropout during testing/prediction)\n",
    "        for i in range(len(self.weights)):\n",
    "            z = np.dot(self.weights[i], activations[-1]) + self.biases[i]\n",
    "            a = self.hidden_activation.forward_pass(z)\n",
    "            activations.append(a)\n",
    "        \n",
    "        return activations[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test was performed before implementing optimisers as seperate class and so it was just done with vanilla batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 completed.\n",
      "Epoch 2/20 completed.\n",
      "Epoch 3/20 completed.\n",
      "Epoch 4/20 completed.\n",
      "Epoch 5/20 completed.\n",
      "Epoch 6/20 completed.\n",
      "Epoch 7/20 completed.\n",
      "Epoch 8/20 completed.\n",
      "Epoch 9/20 completed.\n",
      "Epoch 10/20 completed.\n",
      "Epoch 11/20 completed.\n",
      "Epoch 12/20 completed.\n",
      "Epoch 13/20 completed.\n",
      "Epoch 14/20 completed.\n",
      "Epoch 15/20 completed.\n",
      "Epoch 16/20 completed.\n",
      "Epoch 17/20 completed.\n",
      "Epoch 18/20 completed.\n",
      "Epoch 19/20 completed.\n",
      "Epoch 20/20 completed.\n",
      "Epoch 20/20 - Loss: 1.8393 - Accuracy: 34.13%\n",
      "Predicted class: 7\n",
      "\n",
      "Probability distribution:\n",
      "Class 0: 0.9983\n",
      "Class 1: 0.9932\n",
      "Class 2: 0.9989\n",
      "Class 3: 0.9991\n",
      "Class 4: 0.9994\n",
      "Class 5: 0.9990\n",
      "Class 6: 0.9991\n",
      "Class 7: 0.9996\n",
      "Class 8: 0.9990\n",
      "Class 9: 0.9995\n",
      "\n",
      "True label: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Evaluate on binary classification test set\\ncorrect = 0\\nfor i in range(len(binary_test_images)):\\n    output = nn.run(binary_test_images[i])\\n    predicted_class = 1 if output >= 0.5 else 0\\n    if predicted_class == binary_test_labels[i]:\\n        correct += 1\\n\\naccuracy = correct / len(binary_test_images)\\nprint(f\"Accuracy on binary classification test set: {accuracy * 100:.2f}%\")\\n'"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "train_images = mnist_train.data.numpy().reshape(-1, 28*28) / 255.0  #[0, 1]\n",
    "train_labels = mnist_train.targets.numpy()\n",
    "\n",
    "test_images = mnist_test.data.numpy().reshape(-1, 28*28) / 255.0\n",
    "test_labels = mnist_test.targets.numpy()\n",
    "\n",
    "def one_hot_encode(labels, num_classes=10):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "train_labels_one_hot = one_hot_encode(train_labels)\n",
    "test_labels_one_hot = one_hot_encode(test_labels)\n",
    "\n",
    "sigmoid = SigmoidLayer()\n",
    "softmax = SoftmaxLayer()\n",
    "\n",
    "nn = NeuraNetwork(\n",
    "    in_nodes=784,             #28x28 pixels flattened\n",
    "    out_nodes=10,             #10 digits\n",
    "    hidden_layers=[128, 64],  #two hidden layers\n",
    "    hidden_activation=sigmoid,    #sigmoid for hidden layers\n",
    "    output_activation=softmax,    #softmax for output layer\n",
    "    dropoutRate=0.2,\n",
    "    learning_rate=0.001,\n",
    "    epochs=20\n",
    ")\n",
    "\n",
    "nn.train(train_images, train_labels_one_hot)\n",
    "\n",
    "\n",
    "test_sample = test_images[0]\n",
    "output = nn.run(test_sample)\n",
    "probabilities = output.flatten()  #convert to normal array\n",
    "predicted_class = np.argmax(probabilities)\n",
    "\n",
    "print(\"Predicted class:\", predicted_class)\n",
    "print(\"\\nProbability distribution:\")\n",
    "for i, prob in enumerate(probabilities):\n",
    "    print(f\"Class {i}: {prob:.4f}\")\n",
    "print(f\"\\nTrue label: {test_labels[0]}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(test_images[0].reshape(28, 28), cmap='gray')\n",
    "plt.title(f\"True Label: {test_labels[0]}, Predicted: {predicted_class}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 completed.\n",
      "Epoch 2/20 completed.\n",
      "Epoch 3/20 completed.\n",
      "Epoch 4/20 completed.\n",
      "Epoch 5/20 completed.\n",
      "Epoch 6/20 completed.\n",
      "Epoch 7/20 completed.\n",
      "Epoch 8/20 completed.\n",
      "Epoch 9/20 completed.\n",
      "Epoch 10/20 completed.\n",
      "Epoch 11/20 completed.\n",
      "Epoch 12/20 completed.\n",
      "Epoch 13/20 completed.\n",
      "Epoch 14/20 completed.\n",
      "Epoch 15/20 completed.\n",
      "Epoch 16/20 completed.\n",
      "Epoch 17/20 completed.\n",
      "Epoch 18/20 completed.\n",
      "Epoch 19/20 completed.\n",
      "Epoch 20/20 completed.\n",
      "Epoch 20/20 - Loss: 2.3012 - Accuracy: 11.24%\n",
      "Predicted class: 1\n",
      "\n",
      "Probability distribution:\n",
      "Class 0: 0.0988\n",
      "Class 1: 0.1119\n",
      "Class 2: 0.0995\n",
      "Class 3: 0.1022\n",
      "Class 4: 0.0973\n",
      "Class 5: 0.0905\n",
      "Class 6: 0.0985\n",
      "Class 7: 0.1046\n",
      "Class 8: 0.0978\n",
      "Class 9: 0.0989\n",
      "\n",
      "True label: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn2 = NeuraNetwork(\n",
    "    in_nodes=784,             #28x28 pixels flattened\n",
    "    out_nodes=10,             #10 digits\n",
    "    hidden_layers=[128, 64],  #Two hidden layers\n",
    "    hidden_activation=softmax,    #softmax for hidden layers\n",
    "    output_activation=softmax,    #softmax for output layer\n",
    "    dropoutRate=0.2,\n",
    "    learning_rate=0.001,\n",
    "    epochs=20\n",
    ")\n",
    "\n",
    "nn2.train(train_images, train_labels_one_hot)\n",
    "\n",
    "\n",
    "test_sample = test_images[0]\n",
    "output = nn2.run(test_sample)\n",
    "probabilities = output.flatten()  \n",
    "predicted_class = np.argmax(probabilities)\n",
    "\n",
    "print(\"Predicted class:\", predicted_class)\n",
    "print(\"\\nProbability distribution:\")\n",
    "for i, prob in enumerate(probabilities):\n",
    "    print(f\"Class {i}: {prob:.4f}\")\n",
    "print(f\"\\nTrue label: {test_labels[0]}\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(test_images[0].reshape(28, 28), cmap='gray')\n",
    "plt.title(f\"True Label: {test_labels[0]}, Predicted: {predicted_class}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f) Implement optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be implementing stochastic gradient descent, mini batch descent and momentum batch descent in addition to the default batch descent already present, this will require changing the train() method of the nn class to make it fully parameterizable but I will make the classes for it here based on an interface class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer: #this is like an interface class so i will use the notimplemented error exception because update needs to be done by all the optimisers\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def initialize(self, weights, biases): #only needs to be done by momentum for the velocities\n",
    "        pass\n",
    "    \n",
    "    def update(self, weights, biases, weight_gradients, bias_gradients):\n",
    "        #update botth the weights and the bias vectors that we get from backpropagation\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def should_update_per_sample(self):\n",
    "        #basically a boolean to see if the weights and biases should be updated every sample - for sgd\n",
    "        return False\n",
    "\n",
    "class Batch(Optimizer):\n",
    "    #basic full batch gradient descent just taken from the original train method\n",
    "    def update(self, weights, biases, weight_gradients, bias_gradients):\n",
    "        updated_weights = []\n",
    "        updated_biases = []\n",
    "        \n",
    "        #use zip its the same effect as doing for i in range(len(self.weights)) and referencing the ith elem but is cleaner imo\n",
    "        for w, b, w_grad, b_grad in zip(weights, biases, weight_gradients, bias_gradients):\n",
    "            updated_w = w - self.learning_rate * w_grad\n",
    "            updated_b = b - self.learning_rate * np.sum(b_grad, axis=1, keepdims=True)\n",
    "            \n",
    "            updated_weights.append(updated_w)\n",
    "            updated_biases.append(updated_b)\n",
    "            \n",
    "        return updated_weights, updated_biases\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def should_update_per_sample(self):\n",
    "        #sgd updates the weights after each sample becuase it can lead to quicker convergence\n",
    "        return True\n",
    "    \n",
    "    def update(self, weights, biases, weight_gradients, bias_gradients):\n",
    "        #updates weights and biases using SGD - we update immediately using the gradients from a single sample, so no need to average or accumulate gradients\n",
    "        \n",
    "        updated_weights = []\n",
    "        updated_biases = []\n",
    "        \n",
    "        for w, b, w_grad, b_grad in zip(weights, biases, weight_gradients, bias_gradients):\n",
    "            #direct update\n",
    "            updated_w = w - self.learning_rate * w_grad\n",
    "            updated_b = b - self.learning_rate * b_grad\n",
    "            \n",
    "            updated_weights.append(updated_w)\n",
    "            updated_biases.append(updated_b)\n",
    "            \n",
    "        return updated_weights, updated_biases\n",
    "\n",
    "class Momentum(Optimizer):\n",
    "\n",
    "    def __init__(self, learning_rate, momentum=0.9):\n",
    "        super().__init__(learning_rate)\n",
    "        self.momentum = momentum\n",
    "        self.velocity_w = None\n",
    "        self.velocity_b = None\n",
    "        \n",
    "    def initialize(self, weights, biases):\n",
    "        #initial velocities are zero \n",
    "        self.velocity_w = [np.zeros_like(w) for w in weights]\n",
    "        self.velocity_b = [np.zeros_like(b) for b in biases]\n",
    "    \n",
    "    def update(self, weights, biases, weight_gradients, bias_gradients):\n",
    "        updated_weights = []\n",
    "        updated_biases = []\n",
    "        \n",
    "        for i, (w, b, w_grad, b_grad) in enumerate(zip(weights, biases, weight_gradients, bias_gradients)):\n",
    "            #update the velocity each step\n",
    "            self.velocity_w[i] = self.momentum * self.velocity_w[i] - self.learning_rate * w_grad\n",
    "            self.velocity_b[i] = self.momentum * self.velocity_b[i] - self.learning_rate * np.sum(b_grad, axis=1, keepdims=True)\n",
    "            \n",
    "            updated_w = w + self.velocity_w[i]\n",
    "            updated_b = b + self.velocity_b[i]\n",
    "            \n",
    "            updated_weights.append(updated_w)\n",
    "            updated_biases.append(updated_b)\n",
    "            \n",
    "        return updated_weights, updated_biases #will become the nn class new self.weights and self.biases\n",
    "\n",
    "class MiniBatch(Optimizer):\n",
    "    def __init__(self, learning_rate, batch_size=32):\n",
    "        super().__init__(learning_rate)\n",
    "        self.batch_size = batch_size #now we can deal with smaller batches\n",
    "        \n",
    "    def get_batches(self, input_data, target_data):\n",
    "        #create the batches \n",
    "        n_samples = len(input_data)\n",
    "        indices = np.random.permutation(n_samples) #init the random batch indexes to know where is the start and end \n",
    "        \n",
    "        batch_starts = range(0, n_samples, self.batch_size)\n",
    "        \n",
    "        batches = []\n",
    "        for start in batch_starts:\n",
    "            end = min(start + self.batch_size, n_samples)\n",
    "            batch_indices = indices[start:end]\n",
    "            \n",
    "            input_batch = input_data[batch_indices]\n",
    "            target_batch = target_data[batch_indices]\n",
    "            \n",
    "            batches.append((input_batch, target_batch))\n",
    "            \n",
    "        return batches\n",
    "    \n",
    "    def update(self, weights, biases, weight_gradients, bias_gradients):\n",
    "        updated_weights = []\n",
    "        updated_biases = []\n",
    "        \n",
    "        for w, b, w_grad, b_grad in zip(weights, biases, weight_gradients, bias_gradients):\n",
    "            #average of EACH BATCH\n",
    "            w_grad_mean = np.mean(w_grad, axis=0) if isinstance(w_grad, list) else w_grad\n",
    "            b_grad_mean = np.mean(b_grad, axis=1, keepdims=True) if isinstance(b_grad, list) else b_grad\n",
    "            \n",
    "            #same update as normal batch descent \n",
    "            updated_w = w - self.learning_rate * w_grad_mean\n",
    "            updated_b = b - self.learning_rate * b_grad_mean\n",
    "            \n",
    "            updated_weights.append(updated_w)\n",
    "            updated_biases.append(updated_b)\n",
    "            \n",
    "        return updated_weights, updated_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g) Evaluate different neural network architectures/parameters, present and discuss your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
