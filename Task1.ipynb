{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN3063 Group 2 Coursework Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Imports](#Imports:) \n",
    "2. [Dataset Selection and description](#a-dataset-selection-and-description)\n",
    "3. [Implement sigmoid and ReLU layers](#b-implement-sigmoid-and-relu-layers)\n",
    "4. [Implement dropout](#d-implement-dropout)\n",
    "5. [Implement a fully parameterizable neural network class](#e-implement-a-fully-parametrizable-neural-network-class)\n",
    "6. [Implement optimizer](#f-implement-optimizer)\n",
    "7. [Evaluate different neural network architectures/parameters present and discuss your results](#g-evaluate-different-neural-network-architecturesparameters-present-and-discuss-your-results)\n",
    "8. [Code quality and report and presentation](#h-code-quality-and-report-presentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Dataset selection and description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Implement sigmoid and ReLU layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Implement softmax layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the softmax layer is to convert raw scores (logits) from the neural network into probabilites that sum to 1.\n",
    "\n",
    "Here is the softmax formula: $ f(z)_i = \\frac{(e^z)_i}{\\Sigma_j^K(e^z)_j} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SoftmaxLayer:\n",
    "    self.output = None\n",
    "    self.input = None\n",
    "    self.dinputs = None\n",
    "\n",
    "    def forward_pass(self, inputs):\n",
    "        self.input = inputs\n",
    "        inputs_stable = inputs - np.max(inputs, axis=1, keepdims=True) # stabalize inputs as exponentials grow very large or very small (numerical overflow/underflow)\n",
    "        exp_values = np.exp(inputs_stable)\n",
    "        self.output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "    \n",
    "    def backward_pass(self, dvalues, true_labels):\n",
    "        samples = dvalues.shape[0] # batch size\n",
    "        self.dinputs = (self.output - true_labels) / samples\n",
    "        return self.dinputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Implement dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the dropout layer is to improve the model's generalization and reduce overfitting by randomly temporarily disabling a fraction of the neurons during training, to prevent the model over-relying on specific neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DropoutLayer:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "        self.mask = None\n",
    "\n",
    "    def forward_pass(self, inputs, training=True):\n",
    "        if training:\n",
    "            self.mask = (np.random.rand(*inputs.shape) > self.rate) / (1 - self.rate)\n",
    "            return inputs * self.mask\n",
    "        else:\n",
    "            return inputs\n",
    "    \n",
    "    def backward_pass(self, dvalues):\n",
    "        return dvalues * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Implement a fully parametrizable neural network class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedforward neural network\n",
    "You should implement a fully-connected NN class where with number of hidden\n",
    "layers, units, activation functions can be changed. In addition, you can add dropout or\n",
    "regularizer (L1 or L2). \n",
    "\n",
    "\n",
    "Report the parameters used (update rule, learning rate, decay,\n",
    "epochs, batch size) and include the plots in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
    "    return truncnorm(\n",
    "        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
    "    #Truncated distribution with a specified mean, standard diviation, and bounds\n",
    "\n",
    "class NeuraNetwork:\n",
    "    \n",
    "     def _inti_(self, in_nodes, out_nodes, hidden_nodes, units, active_func, dropoutRate=0.0)\n",
    "        # Intialises the neural network \n",
    "        # Parameters:\n",
    "        # - inNodes: number of in nodes\n",
    "        # - outNode: number of out nodes                                                   \n",
    "        # - hiddenNodes: number of hidden nodes\n",
    "        # - units: represents the number of neurons in a layer\n",
    "        # - active_func: the activation function\n",
    "        # - dropout: dropout rate to reduce overfitting\n",
    "\n",
    "        self.in_nodes=in_nodes\n",
    "        self.out_nodes=out_nodes\n",
    "        self.hidden_nodes=hidden_nodes\n",
    "        self.units=units\n",
    "        self.active_func=active_func\n",
    "        self.dropout=dropout\n",
    "\n",
    "        self.create_weight_matrices()\n",
    "\n",
    "     def create_weight_matrices(self):\n",
    "        \"\"\" A method to initialize the weight matrices of the neural network\"\"\"\n",
    "        rad = 1 / np.sqrt(self.in_nodes)\n",
    "        X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
    "        self.weights_in_hidden = X.rvs((self.hidden_nodes, \n",
    "                                       self.in_nodes))\n",
    "         \n",
    "        rad = 1 / np.sqrt(self.hidden_nodes)\n",
    "        X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
    "        self.weights_hidden_out = X.rvs((self.out_nodes, \n",
    "                                        self.hidden_nodes))\n",
    "\n",
    "     def apply_activation(x, self): \n",
    "         # applies the activate fucntion\n",
    "         if self.active_func == \"sigmoid\": # implements the sigmoid activation function\n",
    "             return 1 / (1 + np.e ** -x)\n",
    "        # add any others\n",
    "         else:\n",
    "             raise ValueError(self.active_func+\" is unsupported\")\n",
    "\n",
    "     def apply_activation_derivative(x, self): \n",
    "          # applies the derivative of the activate fucntion\n",
    "         if self.active_func == \"sigmoid\":# Calculates the derivitive of the sigmoid function, assuming x is already Ïƒ(x)\n",
    "             return  x * (1.0 - x)\n",
    "             # add any others\n",
    "         else:\n",
    "             raise ValueError(active_func+\" is unsupported\")\n",
    "\n",
    "\n",
    "     def train(self, input_vector, target_vector,epochs=100):\n",
    "        # input_vector and target_vector can be tuple, list or ndarray\n",
    "        \n",
    "        input_vector = np.array(input_vector, ndmin=2).T\n",
    "        target_vector = np.array(target_vector, ndmin=2).T\n",
    "    \n",
    "        for i in range(epochs):\n",
    "        \n",
    "            output_vector1 = np.dot(self.weights_in_hidden, input_vector)\n",
    "            output_vector_hidden = apply_activation(output_vector1, self)\n",
    "\n",
    "            output_vector2 = np.dot(self.weights_hidden_out, output_vector_hidden)\n",
    "            output_vector_network = apply_activation(output_vector2, self)\n",
    "            \n",
    "            \n",
    "            output_errors = (target_vector - output_vector_network) \n",
    "            # calculate the weight updates:\n",
    "            derivative_output = apply_activation_derivative (output_vector_network, self)  \n",
    "            tmp = output_errors * derivative_output     \n",
    "            who_update = self.learning_rate * np.dot(tmp, output_vector_hidden.T)\n",
    "\n",
    "            # calculate hidden errors:\n",
    "            hidden_errors = np.dot(self.weights_hidden_out.T, output_errors * derivative_output )\n",
    "\n",
    "            derivative_output = apply_activation_derivative(output_vector_hidden, self)   \n",
    "            tmp = hidden_errors * derivative_output\n",
    "            wih_update = self.learning_rate * np.dot(tmp, input_vector.T)\n",
    "\n",
    "            # update the weights:\n",
    "            self.weights_in_hidden += wih_update\n",
    "            self.weights_hidden_out += who_update\n",
    "                 \n",
    "    def run(self, input_vector):\n",
    "        # input_vector can be tuple, list or ndarray\n",
    "        input_vector = np.array(input_vector, ndmin=2).T\n",
    "        output_vector = np.dot(self.weights_in_hidden, input_vector)\n",
    "        output_vector = apply_activation(output_vector, self)\n",
    "        output_vector = np.dot(self.weights_hidden_out, output_vector)\n",
    "        output_vector = apply_activation(output_vector, self)\n",
    "        \n",
    "        return output_vector.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f) Implement optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g) Evaluate different neural network architectures/parameters, present and discuss your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h) Code quality and report presentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
