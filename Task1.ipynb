{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN3063 Group 2 Coursework Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Imports](#Imports:) \n",
    "2. [Dataset Selection and description](#a-dataset-selection-and-description)\n",
    "3. [Implement sigmoid and ReLU layers](#b-implement-sigmoid-and-relu-layers)\n",
    "4. [Implement dropout](#d-implement-dropout)\n",
    "5. [Implement a fully parameterizable neural network class](#e-implement-a-fully-parametrizable-neural-network-class)\n",
    "6. [Implement optimizer](#f-implement-optimizer)\n",
    "7. [Evaluate different neural network architectures/parameters present and discuss your results](#g-evaluate-different-neural-network-architecturesparameters-present-and-discuss-your-results)\n",
    "8. [Code quality and report and presentation](#h-code-quality-and-report-presentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Dataset selection and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "\n",
    "image, label = mnist_dataset[0]\n",
    "print(\"Label:\" , label)\n",
    "\n",
    "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "plt.axis(\"off\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Implement sigmoid and ReLU layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SigmoidLayer:\n",
    "    def __init__(self):\n",
    "        self.activation_output = None\n",
    "    \n",
    "    def forward_pass(self, x):\n",
    "        self.activation_output = 1 / (1 + np.exp(-x))\n",
    "        return self.activation_output\n",
    "    \n",
    "    def back_pass(self, upstream_gradient):\n",
    "        return (1 - self.activation_output) * self.activation_output * upstream_gradient #return the local gradient \n",
    "    \n",
    "\n",
    "class ReLULayer:\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        self.output = np.maximum(0, x) #relu is like _/ so max of zero and x \n",
    "        return self.output\n",
    "\n",
    "    def back_pass(self, grad_output):\n",
    "        return grad_output * np.where(self.output > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Implement softmax layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the softmax layer is to convert raw scores (logits) from the neural network into probabilites that sum to 1.\n",
    "\n",
    "Here is the softmax formula: $ f(z)_i = \\frac{(e^z)_i}{\\Sigma_j^K(e^z)_j} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SoftmaxLayer:\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "        self.input = None\n",
    "        self.dinputs = None\n",
    "\n",
    "    def forward_pass(self, inputs):\n",
    "        self.input = inputs\n",
    "        inputs_stable = inputs - np.max(inputs, axis=1, keepdims=True) # stabalize inputs as exponentials grow very large or very small (numerical overflow/underflow)\n",
    "        exp_values = np.exp(inputs_stable)\n",
    "        self.output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "    \n",
    "    def backward_pass(self, dvalues, true_labels):\n",
    "        samples = dvalues.shape[0] # batch size\n",
    "        self.dinputs = (self.output - true_labels) / samples\n",
    "        return self.dinputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Implement dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the dropout layer is to improve the model's generalization and reduce overfitting by randomly temporarily disabling a fraction of the neurons during training, to prevent the model over-relying on specific neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DropoutLayer:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "        self.mask = None\n",
    "\n",
    "    def forward_pass(self, inputs, training=True):\n",
    "        if training:\n",
    "            self.mask = (np.random.rand(*inputs.shape) > self.rate) / (1 - self.rate)\n",
    "            return inputs * self.mask\n",
    "        else:\n",
    "            return inputs\n",
    "    \n",
    "    def backward_pass(self, dvalues):\n",
    "        return dvalues * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Implement a fully parametrizable neural network class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedforward neural network\n",
    "You should implement a fully-connected NN class where with number of hidden\n",
    "layers, units, activation functions can be changed. In addition, you can add dropout or\n",
    "regularizer (L1 or L2). \n",
    "\n",
    "\n",
    "Report the parameters used (update rule, learning rate, decay,\n",
    "epochs, batch size) and include the plots in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after training: [[0.40900007]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
    "    return truncnorm(\n",
    "        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
    "    #Truncated distribution with a specified mean, standard diviation, and bounds\n",
    "\n",
    "class NeuraNetwork:\n",
    "    \n",
    "     def __init__(self, in_nodes, out_nodes, hidden_nodes, units, active_func, dropoutRate=0.0, learning_rate=0.01):\n",
    "        # Intialises the neural network \n",
    "        # Parameters:\n",
    "        # - inNodes: number of in nodes\n",
    "        # - outNode: number of out nodes                                                   \n",
    "        # - hiddenNodes: number of hidden nodes\n",
    "        # - units: represents the number of neurons in a layer\n",
    "        # - active_func: the activation function\n",
    "        # - dropout: dropout rate to reduce overfitting\n",
    "\n",
    "        self.in_nodes=in_nodes\n",
    "        self.out_nodes=out_nodes\n",
    "        self.hidden_nodes=hidden_nodes\n",
    "        self.units=units\n",
    "        self.active_func=active_func\n",
    "        self.dropout=dropoutRate\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.create_weight_matrices()\n",
    "\n",
    "     def create_weight_matrices(self):\n",
    "        \"\"\" A method to initialize the weight matrices of the neural network\"\"\"\n",
    "        rad = 1 / np.sqrt(self.in_nodes)\n",
    "        X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
    "        self.weights_in_hidden = X.rvs((self.hidden_nodes, \n",
    "                                       self.in_nodes))\n",
    "         \n",
    "        rad = 1 / np.sqrt(self.hidden_nodes)\n",
    "        X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
    "        self.weights_hidden_out = X.rvs((self.out_nodes, \n",
    "                                        self.hidden_nodes))\n",
    "\n",
    "\n",
    "     def train(self, input_vector, target_vector,epochs=100):\n",
    "        # input_vector and target_vector can be tuple, list or ndarray\n",
    "        \n",
    "        input_vector = np.array(input_vector, ndmin=2).T\n",
    "        target_vector = np.array(target_vector, ndmin=2).T\n",
    "    \n",
    "        for i in range(epochs):\n",
    "        \n",
    "            output_vector1 = np.dot(self.weights_in_hidden, input_vector)\n",
    "            output_vector_hidden = self.active_func.forward_pass(output_vector1)\n",
    "\n",
    "            output_vector2 = np.dot(self.weights_hidden_out, output_vector_hidden)\n",
    "            output_vector_network = self.active_func.forward_pass(output_vector2)\n",
    "            \n",
    "            \n",
    "            output_errors = (target_vector - output_vector_network) \n",
    "            # calculate the weight updates:\n",
    "            derivative_output = self.active_func.back_pass(output_vector_network)  \n",
    "            tmp = output_errors * derivative_output     \n",
    "            who_update = self.learning_rate * np.dot(tmp, output_vector_hidden.T)\n",
    "\n",
    "            # calculate hidden errors:\n",
    "            hidden_errors = np.dot(self.weights_hidden_out.T, output_errors * derivative_output )\n",
    "\n",
    "            derivative_output = self.active_func.back_pass(output_vector_hidden)   \n",
    "            tmp = hidden_errors * derivative_output\n",
    "            wih_update = self.learning_rate * np.dot(tmp, input_vector.T)\n",
    "\n",
    "            # update the weights:\n",
    "            self.weights_in_hidden += wih_update\n",
    "            self.weights_hidden_out += who_update\n",
    "\n",
    "            \n",
    "     def run(self, input_vector):\n",
    "            # input_vector can be tuple, list or ndarray\n",
    "            input_vector = np.array(input_vector, ndmin=2).T\n",
    "            output_vector = np.dot(self.weights_in_hidden, input_vector)\n",
    "            output_vector = self.active_func.forward_pass(output_vector)\n",
    "            output_vector = np.dot(self.weights_hidden_out, output_vector)\n",
    "            output_vector = self.active_func.forward_pass(output_vector)\n",
    "            \n",
    "            return output_vector.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing it out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output after training: [[0.41101859]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sigmoid = SigmoidLayer()\n",
    "dropout = DropoutLayer(0.0)\n",
    "\n",
    "nn = NeuraNetwork(\n",
    "    in_nodes=3,       \n",
    "    out_nodes=1,      \n",
    "    hidden_nodes=4,   \n",
    "    units=4,          \n",
    "    active_func=sigmoid, \n",
    "    learning_rate=0.1\n",
    ")\n",
    "\n",
    "\n",
    "input_vector = [0.5, 0.2, -0.1]   \n",
    "target_vector = [0.4]             \n",
    "\n",
    "nn.train(input_vector, target_vector, epochs=1000)\n",
    "\n",
    "output = nn.run(input_vector)\n",
    "print(\"output after training:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f) Implement optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g) Evaluate different neural network architectures/parameters, present and discuss your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h) Code quality and report presentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
