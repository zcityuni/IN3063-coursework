{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN3063 Group 2 Coursework Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Dataset Selection and description](#a-dataset-selection-and-description)\n",
    "2. [Implement sigmoid and ReLU layers](#b-implement-sigmoid-and-relu-layers)\n",
    "3. [Implement dropout](#d-implement-dropout)\n",
    "4. [Implement a fully parameterizable neural network class](#e-implement-a-fully-parametrizable-neural-network-class)\n",
    "5. [Implement optimizer](#f-implement-optimizer)\n",
    "6. [Evaluate different neural network architectures/parameters present and discuss your results](#g-evaluate-different-neural-network-architecturesparameters-present-and-discuss-your-results)\n",
    "7. [Code quality and report and presentation](#h-code-quality-and-report-presentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Dataset selection and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "\n",
    "image, label = mnist_dataset[0]\n",
    "print(\"Label:\" , label)\n",
    "\n",
    "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "plt.axis(\"off\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Implement sigmoid and ReLU layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SigmoidLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward_pass(self, x):\n",
    "        self.activation_output = 1 / (1 + np.exp(-x))\n",
    "        return self.activation_output\n",
    "    \n",
    "    def backward_pass(self, activation):\n",
    "        return (1 - activation) * activation\n",
    "    \n",
    "\n",
    "class ReLULayer:\n",
    "    def __init__(self):\n",
    "        self.epsilon = 1e-7  #small val so not exact 0\n",
    "        \n",
    "    def forward_pass(self, x):\n",
    "        self.output = np.maximum(self.epsilon, x)\n",
    "        return self.output\n",
    "    \n",
    "    def backward_pass(self, activation):\n",
    "        return np.where(activation > self.epsilon, 1.0, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Implement softmax layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the softmax layer is to convert raw scores (logits) from the neural network into probabilites that sum to 1.\n",
    "\n",
    "Here is the softmax formula: $ f(z)_i = \\frac{(e^z)_i}{\\Sigma_j^K(e^z)_j} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "    \n",
    "class SoftmaxLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward_pass(self, inputs):\n",
    "        #make sure the dimensions are correct shape\n",
    "        if inputs.ndim == 1:\n",
    "            inputs = inputs.reshape(-1, 1)\n",
    "\n",
    "        #for numerical stability\n",
    "        epsilon = 1e-7 #dont use exact zero\n",
    "        shifted_inputs = inputs - np.max(inputs, axis=0, keepdims=True)\n",
    "        exp_values = np.clip(np.exp(shifted_inputs), epsilon, 1e30) #prevent overflow by clipping if too large\n",
    "        #normalise\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=0, keepdims=True)\n",
    "        return probabilities\n",
    "    \n",
    "    def backward_pass(self, activation):\n",
    "        #cross entropy loss\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Implement dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the dropout layer is to improve the model's generalization and reduce overfitting by randomly temporarily disabling a fraction of the neurons during training, to prevent the model over-relying on specific neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DropoutLayer:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "        self.mask = None\n",
    "\n",
    "    def forward_pass(self, inputs, training=True):\n",
    "        if not training or self.rate == 0:\n",
    "            return inputs\n",
    "        self.mask = (np.random.rand(*inputs.shape) > self.rate) / (1 - self.rate)\n",
    "        return inputs * self.mask\n",
    "    \n",
    "    def backward_pass(self, gradient):\n",
    "        return gradient * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Implement a fully parametrizable neural network class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedforward neural network\n",
    "You should implement a fully-connected NN class where with number of hidden\n",
    "layers, units, activation functions can be changed. In addition, you can add dropout or\n",
    "regularizer (L1 or L2). \n",
    "\n",
    "\n",
    "Report the parameters used (update rule, learning rate, decay,\n",
    "epochs, batch size) and include the plots in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f) Implement optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be implementing stochastic gradient descent, mini batch descent and momentum batch descent in addition to the default batch descent already present, this will require changing the train() method of the nn class to make it fully parameterizable but I will make the classes for it here based on an interface class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer: #this is like an interface class so i will use the notimplemented error exception because update needs to be done by all the optimizers\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def initialize(self, weights, biases): #only needs to be done by momentum for the velocities\n",
    "        pass\n",
    "    \n",
    "    def update(self, weights, biases, weight_gradients, bias_gradients):\n",
    "        #update botth the weights and the bias vectors that we get from backpropagation\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def should_update_per_sample(self):\n",
    "        #basically a boolean to see if the weights and biases should be updated every sample - for sgd\n",
    "        return False\n",
    "\n",
    "class Batch(Optimizer):\n",
    "    #basic full batch gradient descent just taken from the original train method\n",
    "    def update(self, weights, biases, weight_gradients, bias_gradients):\n",
    "        updated_weights = []\n",
    "        updated_biases = []\n",
    "\n",
    "        #my gradients were exploding causing them to overflow and become nan\n",
    "        max_grad_norm = 1.0\n",
    "        #norm of sum of gradients \n",
    "        grad_norm = np.sqrt(sum([np.sum(grad**2) for grad in weight_gradients]))\n",
    "        \n",
    "        #clip if the norm is too big\n",
    "        if grad_norm > max_grad_norm:\n",
    "            scale = max_grad_norm / (grad_norm + 1e-7)\n",
    "            weight_gradients = [grad * scale for grad in weight_gradients]\n",
    "            bias_gradients = [grad * scale for grad in bias_gradients]\n",
    "\n",
    "        #use zip its the same effect as doing for i in range(len(self.weights)) and referencing the ith elem but is cleaner imo\n",
    "        for w, b, w_grad, b_grad in zip(weights, biases, weight_gradients, bias_gradients):\n",
    "            updated_w = w - self.learning_rate * w_grad\n",
    "            updated_b = b - self.learning_rate * np.sum(b_grad, axis=1, keepdims=True)\n",
    "            \n",
    "            updated_weights.append(updated_w)\n",
    "            updated_biases.append(updated_b)\n",
    "            \n",
    "        return updated_weights, updated_biases\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def should_update_per_sample(self):\n",
    "        #sgd updates the weights after each sample becuase it can lead to quicker convergence\n",
    "        return True\n",
    "    \n",
    "    def update(self, weights, biases, weight_gradients, bias_gradients):\n",
    "        #updates weights and biases using SGD - we update immediately using the gradients from a single sample, so no need to average or accumulate gradients\n",
    "        \n",
    "        updated_weights = []\n",
    "        updated_biases = []\n",
    "\n",
    "        max_grad_norm = 1.0\n",
    "        grad_norm = np.sqrt(sum([np.sum(grad**2) for grad in weight_gradients]))\n",
    "        \n",
    "        if grad_norm > max_grad_norm:\n",
    "            scale = max_grad_norm / (grad_norm + 1e-7)\n",
    "            weight_gradients = [grad * scale for grad in weight_gradients]\n",
    "            bias_gradients = [grad * scale for grad in bias_gradients]\n",
    "        \n",
    "        for w, b, w_grad, b_grad in zip(weights, biases, weight_gradients, bias_gradients):\n",
    "            #direct update\n",
    "            updated_w = w - self.learning_rate * w_grad\n",
    "            updated_b = b - self.learning_rate * b_grad\n",
    "            \n",
    "            updated_weights.append(updated_w)\n",
    "            updated_biases.append(updated_b)\n",
    "            \n",
    "        return updated_weights, updated_biases\n",
    "\n",
    "class Momentum(Optimizer):\n",
    "\n",
    "    def __init__(self, learning_rate, momentum=0.9):\n",
    "        super().__init__(learning_rate)\n",
    "        self.momentum = momentum\n",
    "        self.velocity_w = None\n",
    "        self.velocity_b = None\n",
    "        \n",
    "    def initialize(self, weights, biases):\n",
    "        #initial velocities are zero \n",
    "        self.velocity_w = [np.zeros_like(w) for w in weights]\n",
    "        self.velocity_b = [np.zeros_like(b) for b in biases]\n",
    "    \n",
    "    def update(self, weights, biases, weight_gradients, bias_gradients):\n",
    "        updated_weights = []\n",
    "        updated_biases = []\n",
    "        \n",
    "        max_norm = 1.0\n",
    "        total_norm = np.sqrt(sum(np.sum(grad**2) for grad in weight_gradients))\n",
    "        \n",
    "        if total_norm > max_norm:\n",
    "            scale = max_norm / (total_norm + 1e-7)\n",
    "            weight_gradients = [grad * scale for grad in weight_gradients]\n",
    "            bias_gradients = [grad * scale for grad in bias_gradients]\n",
    "\n",
    "        for i, (w, b, w_grad, b_grad) in enumerate(zip(weights, biases, weight_gradients, bias_gradients)):\n",
    "            #update the velocities every step\n",
    "            self.velocity_w[i] = self.momentum * self.velocity_w[i] - self.learning_rate * w_grad\n",
    "            self.velocity_b[i] = self.momentum * self.velocity_b[i] - self.learning_rate * np.sum(b_grad, axis=1, keepdims=True)\n",
    "            \n",
    "            updated_w = w + self.velocity_w[i]\n",
    "            updated_b = b + self.velocity_b[i]\n",
    "            \n",
    "            updated_weights.append(updated_w)\n",
    "            updated_biases.append(updated_b)\n",
    "            \n",
    "        return updated_weights, updated_biases #will become the nn class new self.weights and self.biases\n",
    "\n",
    "class MiniBatch(Optimizer):\n",
    "    def __init__(self, learning_rate, batch_size=32):\n",
    "        super().__init__(learning_rate)\n",
    "        self.batch_size = batch_size #now we can deal with smaller batches\n",
    "        \n",
    "    def get_batches(self, input_data, target_data):\n",
    "        #create the batches \n",
    "        n_samples = len(input_data)\n",
    "        indices = np.random.permutation(n_samples) #init the random batch indexes to know where is the start and end \n",
    "        \n",
    "        batch_starts = range(0, n_samples, self.batch_size)\n",
    "        \n",
    "        batches = []\n",
    "        for start in batch_starts:\n",
    "            end = min(start + self.batch_size, n_samples)\n",
    "            batch_indices = indices[start:end]\n",
    "            \n",
    "            input_batch = input_data[batch_indices]\n",
    "            target_batch = target_data[batch_indices]\n",
    "            \n",
    "            batches.append((input_batch, target_batch))\n",
    "            \n",
    "        return batches\n",
    "    \n",
    "    def update(self, weights, biases, weight_gradients, bias_gradients):\n",
    "        updated_weights = []\n",
    "        updated_biases = []\n",
    "\n",
    "        #for mini batch its done on the MEAN of the gradients same as list comprehension way above but we take the mean instrad\n",
    "        w_grads_mean = []\n",
    "        b_grads_mean = []\n",
    "\n",
    "        for w_grad in weight_gradients:\n",
    "            if isinstance(w_grad, list):\n",
    "                mean_gradient = np.mean(w_grad, axis=0)\n",
    "            else:\n",
    "                mean_gradient = w_grad\n",
    "            w_grads_mean.append(mean_gradient)\n",
    "\n",
    "        for b_grad in bias_gradients:\n",
    "            if isinstance(b_grad, list):\n",
    "                mean_gradient = np.mean(b_grad, axis=1, keepdims=True)\n",
    "            else:\n",
    "                mean_gradient = b_grad\n",
    "            b_grads_mean.append(mean_gradient)\n",
    "        \n",
    "        #clip the means\n",
    "        max_norm = 1.0\n",
    "        total_norm = np.sqrt(sum(np.sum(grad**2) for grad in w_grads_mean))\n",
    "        \n",
    "        if total_norm > max_norm:\n",
    "            scale = max_norm / (total_norm + 1e-7)\n",
    "            w_grads_mean = [grad * scale for grad in w_grads_mean]\n",
    "            b_grads_mean = [grad * scale for grad in b_grads_mean]\n",
    "        \n",
    "        #update w the clipped gradients\n",
    "        for w, b, w_grad, b_grad in zip(weights, biases, w_grads_mean, b_grads_mean):\n",
    "            updated_w = w - self.learning_rate * w_grad\n",
    "            updated_b = b - self.learning_rate * b_grad\n",
    "            \n",
    "            updated_weights.append(updated_w)\n",
    "            updated_biases.append(updated_b)\n",
    "            \n",
    "        return updated_weights, updated_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "class NeuraNetwork:\n",
    "    \n",
    "     def __init__(self, in_nodes, out_nodes, hidden_layers, hidden_activation, output_activation, dropoutRate=0.0, learning_rate=0.001, regularization=None, lambda_=0.01, epochs=100, optimizer='bgd', batch_size=32, decay_rate=0.95, decay_steps=5, min_lr=1e-5):\n",
    "        # Intialises the neural network \n",
    "        #Parameters\n",
    "        # inNodes: number of input nodes\n",
    "        # outNodes: number of output nodes\n",
    "        # hidden_layers: list of number of neurons in each hidden layer. eg [128, 64, 32]\n",
    "        # hidden_activation: activation function (uses the forward_pass and backward_pass methods) for hidden layers\n",
    "        # output_activation: activation function for output layer \n",
    "        # dropoutRate: dropout rate to reduce overfitting\n",
    "        # learning_rate: learning rate for training\n",
    "        # regularization: type of regularization l1 or l2\n",
    "        # lambda_: regularization strength\n",
    "        # epochs : the number of epochs that will run over the set\n",
    "        # optimizer : the selected optimizer out of bdg, sgd, minibgd, momentum\n",
    "        # batch_size : size for minibatch optimiser\n",
    "        # decay_rate : rate that the learning rate decays by (decrease the learning rate as we get closer to convergence)\n",
    "        # decay-steps : amount of step intervals we should apply the decay rate\n",
    "        # min_lr : the smallest learning rate that will be used throught training\n",
    "    \n",
    "        self.in_nodes = in_nodes\n",
    "        self.out_nodes = out_nodes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.output_activation = output_activation\n",
    "        self.dropoutRate = dropoutRate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "        self.lambda_ = lambda_\n",
    "        self.epochs = epochs\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.decay_rate = decay_rate\n",
    "        self.decay_steps = decay_steps\n",
    "        self.min_lr = min_lr\n",
    "        self.create_weight_matrices()\n",
    "        self.dropout = DropoutLayer(self.dropoutRate)\n",
    "\n",
    "        if(self.optimizer == \"bgd\"):\n",
    "            self.optimizer = Batch(self.learning_rate)\n",
    "        elif(self.optimizer == \"sgd\"):\n",
    "            self.optimizer = SGD(self.learning_rate)\n",
    "        elif(self.optimizer == \"minibgd\"):\n",
    "            self.optimizer = MiniBatch(self.learning_rate, self.batch_size)\n",
    "        elif(self.optimizer == \"momentum\"):\n",
    "            self.optimizer == Momentum(self.learning_rate)\n",
    "        else:\n",
    "            raise Exception(\"invalid optimizer choice\")\n",
    "\n",
    "     def truncated_normal(self, mean=0, sd=1, low=0, upp=10):\n",
    "        return truncnorm(\n",
    "            (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
    "        #Truncated distribution with a specified mean, standard diviation, and bounds\n",
    "\n",
    "     def create_weight_matrices(self):\n",
    "        \"\"\"method to initialize the weight matrices of the neural network\"\"\"\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        '''\n",
    "        #init input to first hiddn layer - make it random so its not all zeroes\n",
    "        rad = 1 / np.sqrt(self.in_nodes) #type of xavier init - to prevent activations and gradients from growing in a exponential way between layers\n",
    "        X = self.truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
    "        w1 = (X.rvs((self.hidden_layers[0], self.in_nodes))) #initial dimensions is num of neurons in first hidden layer by amount of features\n",
    "        self.weights.append(w1 / np.sqrt(np.sum(w1 * w1)))  #normalise the wieght\n",
    "        self.biases.append(np.zeros((self.hidden_layers[0], 1))) #inital bias col vector \n",
    "        \n",
    "        #init hidden layer to next hidden layer for all hidden layers\n",
    "        for i in range(1, len(self.hidden_layers)):\n",
    "            rad = 1 / np.sqrt(self.hidden_layers[i-1])\n",
    "            X = self.truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
    "            w = (X.rvs((self.hidden_layers[i], self.hidden_layers[i-1])))\n",
    "            self.weights.append(w / np.sqrt(np.sum(w * w))) #normalise\n",
    "            self.biases.append(np.zeros((self.hidden_layers[i], 1)))\n",
    "\n",
    "        #init last layer to output layer\n",
    "        rad = 1 / np.sqrt(self.hidden_layers[-1])\n",
    "        X = self.truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
    "        w_out = (X.rvs((self.out_nodes, self.hidden_layers[-1]))) #init dimension for output layer\n",
    "        self.weights.append(w_out / np.sqrt(np.sum(w_out * w_out))) #normalise\n",
    "        self.biases.append(np.zeros((self.out_nodes, 1)))\n",
    "        '''\n",
    "        '''\n",
    "       # Adding a scaling factor to make initialization more conservative\n",
    "        conservative_factor = 0.01\n",
    "    \n",
    "        # Input to first hidden layer\n",
    "        scale = conservative_factor * np.sqrt(2.0 / self.in_nodes)\n",
    "        self.weights.append(np.random.randn(self.hidden_layers[0], self.in_nodes) * scale)\n",
    "        self.biases.append(np.zeros((self.hidden_layers[0], 1)))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(1, len(self.hidden_layers)):\n",
    "            scale = conservative_factor * np.sqrt(2.0 / self.hidden_layers[i-1])\n",
    "            self.weights.append(np.random.randn(self.hidden_layers[i], self.hidden_layers[i-1]) * scale)\n",
    "            self.biases.append(np.zeros((self.hidden_layers[i], 1)))\n",
    "        \n",
    "        # Output layer\n",
    "        scale = conservative_factor * np.sqrt(2.0 / self.hidden_layers[-1])\n",
    "        self.weights.append(np.random.randn(self.out_nodes, self.hidden_layers[-1]) * scale)\n",
    "        self.biases.append(np.zeros((self.out_nodes, 1)))\n",
    "        '''\n",
    "        \n",
    "        #back to xavier\n",
    "        fan_in = self.in_nodes\n",
    "        lim = 1 / np.sqrt(fan_in)\n",
    "        W1 = np.random.uniform(-lim, lim, size=(self.hidden_layers[0], fan_in))\n",
    "        self.weights.append(W1)\n",
    "        self.biases.append(np.zeros((self.hidden_layers[0], 1))) \n",
    "\n",
    "        for i in range(1, len(self.hidden_layers)):\n",
    "            fan_in = self.hidden_layers[i-1]\n",
    "            lim = 1 / np.sqrt(fan_in)\n",
    "            W = np.random.uniform(-lim, lim, size=(self.hidden_layers[i], fan_in))\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(np.zeros((self.hidden_layers[i], 1)))\n",
    "\n",
    "        fan_in = self.hidden_layers[-1]\n",
    "        lim = 1 / np.sqrt(fan_in)\n",
    "        W_out = np.random.uniform(-lim, lim, size=(self.out_nodes, fan_in))\n",
    "        self.weights.append(W_out)\n",
    "        self.biases.append(np.zeros((self.out_nodes, 1)))\n",
    "            \n",
    "     def apply_regularization(self, weights):\n",
    "            \"\"\"applies L1 or L2 regularization.\"\"\"\n",
    "            if self.regularization == 'L1':\n",
    "                return self.lambda_ * np.sign(weights)\n",
    "            elif self.regularization == 'L2':\n",
    "                return self.lambda_ * weights\n",
    "            return 0   \n",
    "     \n",
    "     def monitor_weights(self):\n",
    "        \"\"\"monitor weight statistics with layer-specific thresholds\"\"\"\n",
    "        for i, w in enumerate(self.weights):\n",
    "            if np.any(np.isnan(w)):\n",
    "                print(f\"layer {i} weights contain NaN values!!!\")\n",
    "                return False\n",
    "            \n",
    "            magnitude = np.sqrt(np.mean(w * w))#mean sum for better scaling\n",
    "            threshold = 2.0\n",
    "            \n",
    "            if magnitude > threshold:\n",
    "                print(f\"layer {i} weights RMS magnitude ({magnitude:.4f}) is too large\")\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "     \n",
    "\n",
    "     def forward_pass(self, input_vector, training=True):\n",
    "        \"\"\"perform a forward pass, each layer has its own weights and bias matrices as init above\"\"\"\n",
    "\n",
    "        if not self.monitor_weights():\n",
    "            raise ValueError(\"weight instability detected!\")\n",
    "     \n",
    "        activations = [input_vector] #init the activations list to be the input vector \n",
    "        for i in range(len(self.weights) -1 ): #for each layer in the nn up to output layer\n",
    "            #calculate the z value of the neuron which is the dot product of This Layer's weights and the activations of the previous layer added with bias col vector [Z = W(L) * a(L-1) + b(L)]\n",
    "            z = np.dot(self.weights[i], activations[-1]) + self.biases[i] \n",
    "\n",
    "            if np.any(np.isnan(z)):\n",
    "                print(f\"NaN detected after layer {i} weights multiplication\")\n",
    "                print(f\"Weight stats - min: {np.min(self.weights[i])}, max: {np.max(self.weights[i])}\")\n",
    "                print(f\"Previous activation stats - min: {np.min(activations[-1])}, max: {np.max(activations[-1])}\")\n",
    "\n",
    "            a = self.hidden_activation.forward_pass(z) #calc the activation value with forward_pass method of the activation func sigmoid or ReLU or softMax\n",
    "            if training and self.dropoutRate > 0:\n",
    "                a = self.dropout.forward_pass(a, training) #apply dropout mask if training\n",
    "            activations.append(a) #update the list of activations, the final nn output is activations[-1] \n",
    "        \n",
    "        #output layer activations\n",
    "        z_final = np.dot(self.weights[-1], activations[-1]) + self.biases[-1]\n",
    "        a_final = self.output_activation.forward_pass(z_final)\n",
    "        activations.append(a_final)\n",
    "\n",
    "        return activations\n",
    "    \n",
    "\n",
    "     def backpropagation(self, activations, target_vector):\n",
    "        \"\"\"perform backward pass to compute gradients\"\"\"\n",
    "        error = [target_vector - activations[-1]] #error for each layer \n",
    "        gradients_w = []\n",
    "        gradients_b = []\n",
    "\n",
    "        #starting from the final layer back to the first layer \n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            error[-1] = np.clip(error[-1], -1.0, 1.0) #clip to prevent extrme \n",
    "            #ok so derivative of output error with respect to weight dL/dW(L) = (a(L) - y) * a(L)(1-a(L)) * a(L-1) (in the case of sigmoid)\n",
    "            #error[-1] contains (a(L) - y)\n",
    "            #so we multiply it by the derivative of the activation function a(L)(1-a(L)) for sigmoid \n",
    "            #so delta = dL/dZ = dL/dA * dA/dZ. \n",
    "            if i == len(self.weights) - 1:  #the output layer is first \n",
    "                delta = error[-1] * self.output_activation.backward_pass(activations[i+1])\n",
    "            else:\n",
    "                delta = error[-1] * self.hidden_activation.backward_pass(activations[i+1])\n",
    "            \n",
    "            #to get the gradient dL/dw(L) we need to multiply dL/dZ (delta) by dZ(L)/dW(L). Which is same as delta * a(L-1) as well as any regularization that prevents overfitting\n",
    "            current_gradient = np.dot(delta, activations[i].T) + self.apply_regularization(self.weights[i])\n",
    "            grad_norm = np.sqrt(np.sum(current_gradient * current_gradient))\n",
    "            if grad_norm > 1.0:  # If gradient magnitude is too large\n",
    "                current_gradient = current_gradient / (grad_norm + 1e-7)\n",
    "            gradients_w.append(current_gradient)\n",
    "            gradients_b.append(delta) # dL/db(L) = dL/dZ(L) * dZ(L)/db(L) = delta * 1 = delta\n",
    "            \n",
    "            if i > 0:\n",
    "                backward_error = np.dot(self.weights[i].T, delta)\n",
    "                error_norm = np.sqrt(np.sum(backward_error * backward_error)) #prevent it growing too large\n",
    "                if error_norm > 1.0:\n",
    "                    backward_error = backward_error / (error_norm + 1e-7)\n",
    "                error.append(backward_error)\n",
    "\n",
    "\n",
    "        gradients_w.reverse()\n",
    "        gradients_b.reverse()\n",
    "        return gradients_w, gradients_b\n",
    "\n",
    "     def train(self, input_data, target_data):\n",
    "        \"\"\"\n",
    "        train with basic gradient descent over given number of epoches\n",
    "        \"\"\"\n",
    "        input_data = np.array(input_data)\n",
    "        target_data = np.array(target_data)\n",
    "\n",
    "        #keep track of statistics from training  - using list for each one so i can graph it later\n",
    "        history = {\n",
    "            'loss': [],\n",
    "            'accuracy': [],\n",
    "            'val_loss': [],\n",
    "            'val_accuracy': [],\n",
    "            'learning_rate': [] \n",
    "        }\n",
    "\n",
    "        initial_lr = self.optimizer.learning_rate #inital learning rate is one we specifed, this wil change as decay happnes\n",
    "\n",
    "        #init the optimiser if its momentum - will do nothing if its a different optimiser\n",
    "        self.optimizer.initialize(self.weights, self.biases)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            #applying the learning rate decay formula\n",
    "            current_lr = initial_lr * (self.decay_rate ** (epoch // self.decay_steps))\n",
    "            current_lr = max(current_lr, self.min_lr)  #make sure it does not go below the min lr \n",
    "            self.optimizer.learning_rate = current_lr\n",
    "\n",
    "            epoch_loss = 0\n",
    "            n_samples = 0\n",
    "            correct_predictions = 0\n",
    "\n",
    "            #prep and init the batches if we are using mini batch otherwise apply over whole batch\n",
    "            if isinstance(self.optimizer, MiniBatch):\n",
    "                batches = self.optimizer.get_batches(input_data, target_data)\n",
    "            else:\n",
    "                batches = [(input_data, target_data)]\n",
    "            \n",
    "            #for every batch (only 1 in case that we arent using minibatch)\n",
    "            for batch_input, batch_target in batches:\n",
    "                batch_size = len(batch_input)\n",
    "                n_samples += batch_size\n",
    "                \n",
    "                #init the gradients (if we arent using sgd)\n",
    "                batch_grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "                batch_grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "                \n",
    "                #do a full forward pass and backpropagation and update the weights for each batch/sample\n",
    "                for i in range(batch_size):\n",
    "                    #make sure theyre in the correct dimensions\n",
    "                    input_vector = np.expand_dims(batch_input[i], axis=1)\n",
    "                    target_vector = np.expand_dims(batch_target[i], axis=1)\n",
    "                    \n",
    "                    #do the forward pass\n",
    "                    activations = self.forward_pass(input_vector, training=True)\n",
    "                    \n",
    "                    #calc cross-entropy loss\n",
    "                    epsilon = 1e-15\n",
    "                    output = np.clip(activations[-1], epsilon, 1 - epsilon)\n",
    "                    loss = -np.sum(target_vector * np.log(output))\n",
    "                    epoch_loss += loss\n",
    "                    \n",
    "                    #track the accuracy\n",
    "                    predicted = np.argmax(activations[-1])\n",
    "                    true_label = np.argmax(target_vector)\n",
    "                    if predicted == true_label:\n",
    "                        correct_predictions += 1\n",
    "                    \n",
    "                    #backpropagation\n",
    "                    gradients_w, gradients_b = self.backpropagation(activations, target_vector)\n",
    "                    \n",
    "                    if self.optimizer.should_update_per_sample():\n",
    "                        #update weights directly for sgd after each sample\n",
    "                        self.weights, self.biases = self.optimizer.update(\n",
    "                            self.weights, self.biases, gradients_w, gradients_b\n",
    "                        )\n",
    "                    else:\n",
    "                        #for the other optimizers we update our batch gradient\n",
    "                        for j in range(len(self.weights)):\n",
    "                            batch_grad_w[j] += gradients_w[j]\n",
    "                            batch_grad_b[j] += gradients_b[j]\n",
    "                \n",
    "                #update each layers weights (non sgd)\n",
    "                if not self.optimizer.should_update_per_sample():\n",
    "                    self.weights, self.biases = self.optimizer.update(\n",
    "                        self.weights, self.biases, batch_grad_w, batch_grad_b\n",
    "                    )\n",
    "            \n",
    "            #calc loss and accuracy\n",
    "            avg_loss = epoch_loss / n_samples\n",
    "            accuracy = (correct_predictions / n_samples) * 100\n",
    "            \n",
    "            #add it to the metric history\n",
    "            history['loss'].append(avg_loss)\n",
    "            history['accuracy'].append(accuracy)\n",
    "            history['learning_rate'].append(current_lr)\n",
    "            \n",
    "            print(f\"epoch {epoch + 1}/{self.epochs} - loss: {avg_loss} - accuracy: {accuracy}% - LR: {current_lr}\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "            \n",
    "     def run(self, input_data, targets=None):\n",
    "        \"\"\"\n",
    "        prediction method, does a forward pass through the network on the input vector and returns the final activation vector\n",
    "        \"\"\"\n",
    "        #make sure the input vector dimensions correct format\n",
    "        if input_data.ndim == 1:\n",
    "            input_data = np.expand_dims(input_data, axis=0)\n",
    "        \n",
    "        batch_size = len(input_data)\n",
    "        predictions = []\n",
    "        metrics = {'loss': 0, 'accuracy': 0} if targets is not None else None\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            input_vector = np.expand_dims(input_data[i], axis=1)\n",
    "            \n",
    "            #forward pass (no dropout during testing)\n",
    "            activations = [input_vector]\n",
    "            for j in range(len(self.weights)):\n",
    "                z = np.dot(self.weights[j], activations[-1]) + self.biases[j]\n",
    "                a = self.hidden_activation.forward_pass(z) if j < len(self.weights)-1 else self.output_activation.forward_pass(z)\n",
    "                activations.append(a)\n",
    "            \n",
    "            predictions.append(activations[-1])\n",
    "            \n",
    "            #calc metrics\n",
    "            if targets is not None:\n",
    "                target_vector = np.expand_dims(targets[i], axis=1)\n",
    "                \n",
    "                #cross-entropy loss\n",
    "                epsilon = 1e-15\n",
    "                output = np.clip(activations[-1], epsilon, 1 - epsilon)\n",
    "                loss = -np.sum(target_vector * np.log(output))\n",
    "                metrics['loss'] += loss\n",
    "                \n",
    "                #accuracy\n",
    "                predicted = np.argmax(activations[-1])\n",
    "                true_label = np.argmax(target_vector)\n",
    "                metrics['accuracy'] += (predicted == true_label)\n",
    "        \n",
    "        predictions = np.array(predictions).squeeze()\n",
    "        if metrics is not None:\n",
    "            metrics['loss'] /= batch_size\n",
    "            metrics['accuracy'] = (metrics['accuracy'] / batch_size) * 100\n",
    "        \n",
    "        if metrics is not None:\n",
    "            return (predictions, metrics)\n",
    "        else:\n",
    "            return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test was performed before implementing optimisers as seperate class and so it was just done with vanilla batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 completed.\n",
      "Epoch 2/20 completed.\n",
      "Epoch 3/20 completed.\n",
      "Epoch 4/20 completed.\n",
      "Epoch 5/20 completed.\n",
      "Epoch 6/20 completed.\n",
      "Epoch 7/20 completed.\n",
      "Epoch 8/20 completed.\n",
      "Epoch 9/20 completed.\n",
      "Epoch 10/20 completed.\n",
      "Epoch 11/20 completed.\n",
      "Epoch 12/20 completed.\n",
      "Epoch 13/20 completed.\n",
      "Epoch 14/20 completed.\n",
      "Epoch 15/20 completed.\n",
      "Epoch 16/20 completed.\n",
      "Epoch 17/20 completed.\n",
      "Epoch 18/20 completed.\n",
      "Epoch 19/20 completed.\n",
      "Epoch 20/20 completed.\n",
      "Epoch 20/20 - Loss: 1.8393 - Accuracy: 34.13%\n",
      "Predicted class: 7\n",
      "\n",
      "Probability distribution:\n",
      "Class 0: 0.9983\n",
      "Class 1: 0.9932\n",
      "Class 2: 0.9989\n",
      "Class 3: 0.9991\n",
      "Class 4: 0.9994\n",
      "Class 5: 0.9990\n",
      "Class 6: 0.9991\n",
      "Class 7: 0.9996\n",
      "Class 8: 0.9990\n",
      "Class 9: 0.9995\n",
      "\n",
      "True label: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Evaluate on binary classification test set\\ncorrect = 0\\nfor i in range(len(binary_test_images)):\\n    output = nn.run(binary_test_images[i])\\n    predicted_class = 1 if output >= 0.5 else 0\\n    if predicted_class == binary_test_labels[i]:\\n        correct += 1\\n\\naccuracy = correct / len(binary_test_images)\\nprint(f\"Accuracy on binary classification test set: {accuracy * 100:.2f}%\")\\n'"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "train_images = mnist_train.data.numpy().reshape(-1, 28*28) / 255.0  #[0, 1]\n",
    "train_labels = mnist_train.targets.numpy()\n",
    "\n",
    "test_images = mnist_test.data.numpy().reshape(-1, 28*28) / 255.0\n",
    "test_labels = mnist_test.targets.numpy()\n",
    "\n",
    "def one_hot_encode(labels, num_classes=10):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "train_labels_one_hot = one_hot_encode(train_labels)\n",
    "test_labels_one_hot = one_hot_encode(test_labels)\n",
    "\n",
    "sigmoid = SigmoidLayer()\n",
    "softmax = SoftmaxLayer()\n",
    "\n",
    "nn = NeuraNetwork(\n",
    "    in_nodes=784,             #28x28 pixels flattened\n",
    "    out_nodes=10,             #10 digits\n",
    "    hidden_layers=[128, 64],  #two hidden layers\n",
    "    hidden_activation=sigmoid,    #sigmoid for hidden layers\n",
    "    output_activation=softmax,    #softmax for output layer\n",
    "    dropoutRate=0.2,\n",
    "    learning_rate=0.001,\n",
    "    epochs=20\n",
    ")\n",
    "\n",
    "nn.train(train_images, train_labels_one_hot)\n",
    "\n",
    "\n",
    "test_sample = test_images[0]\n",
    "output = nn.run(test_sample)\n",
    "probabilities = output.flatten()  #convert to normal array\n",
    "predicted_class = np.argmax(probabilities)\n",
    "\n",
    "print(\"Predicted class:\", predicted_class)\n",
    "print(\"\\nProbability distribution:\")\n",
    "for i, prob in enumerate(probabilities):\n",
    "    print(f\"Class {i}: {prob:.4f}\")\n",
    "print(f\"\\nTrue label: {test_labels[0]}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(test_images[0].reshape(28, 28), cmap='gray')\n",
    "plt.title(f\"True Label: {test_labels[0]}, Predicted: {predicted_class}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 completed.\n",
      "Epoch 2/20 completed.\n",
      "Epoch 3/20 completed.\n",
      "Epoch 4/20 completed.\n",
      "Epoch 5/20 completed.\n",
      "Epoch 6/20 completed.\n",
      "Epoch 7/20 completed.\n",
      "Epoch 8/20 completed.\n",
      "Epoch 9/20 completed.\n",
      "Epoch 10/20 completed.\n",
      "Epoch 11/20 completed.\n",
      "Epoch 12/20 completed.\n",
      "Epoch 13/20 completed.\n",
      "Epoch 14/20 completed.\n",
      "Epoch 15/20 completed.\n",
      "Epoch 16/20 completed.\n",
      "Epoch 17/20 completed.\n",
      "Epoch 18/20 completed.\n",
      "Epoch 19/20 completed.\n",
      "Epoch 20/20 completed.\n",
      "Epoch 20/20 - Loss: 2.3012 - Accuracy: 11.24%\n",
      "Predicted class: 1\n",
      "\n",
      "Probability distribution:\n",
      "Class 0: 0.0988\n",
      "Class 1: 0.1119\n",
      "Class 2: 0.0995\n",
      "Class 3: 0.1022\n",
      "Class 4: 0.0973\n",
      "Class 5: 0.0905\n",
      "Class 6: 0.0985\n",
      "Class 7: 0.1046\n",
      "Class 8: 0.0978\n",
      "Class 9: 0.0989\n",
      "\n",
      "True label: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn2 = NeuraNetwork(\n",
    "    in_nodes=784,             #28x28 pixels flattened\n",
    "    out_nodes=10,             #10 digits\n",
    "    hidden_layers=[128, 64],  #Two hidden layers\n",
    "    hidden_activation=softmax,    #softmax for hidden layers\n",
    "    output_activation=softmax,    #softmax for output layer\n",
    "    dropoutRate=0.2,\n",
    "    learning_rate=0.001,\n",
    "    epochs=20\n",
    ")\n",
    "\n",
    "nn2.train(train_images, train_labels_one_hot)\n",
    "\n",
    "\n",
    "test_sample = test_images[0]\n",
    "output = nn2.run(test_sample)\n",
    "probabilities = output.flatten()  \n",
    "predicted_class = np.argmax(probabilities)\n",
    "\n",
    "print(\"Predicted class:\", predicted_class)\n",
    "print(\"\\nProbability distribution:\")\n",
    "for i, prob in enumerate(probabilities):\n",
    "    print(f\"Class {i}: {prob:.4f}\")\n",
    "print(f\"\\nTrue label: {test_labels[0]}\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(test_images[0].reshape(28, 28), cmap='gray')\n",
    "plt.title(f\"True Label: {test_labels[0]}, Predicted: {predicted_class}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g) Evaluate different neural network architectures/parameters, present and discuss your results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the tests conducted after the optimizer class was added and made paramaterisable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training...\n",
      "epoch 1/50 - loss: 2.3114010982214124 - accuracy: 9.037037037037036% - LR: 0.001\n",
      "epoch 2/50 - loss: 2.3378488030335367 - accuracy: 9.074074074074074% - LR: 0.001\n",
      "epoch 3/50 - loss: 2.4558242091737577 - accuracy: 5.4185185185185185% - LR: 0.001\n",
      "epoch 4/50 - loss: 4.781972400877047 - accuracy: 8.540740740740741% - LR: 0.001\n",
      "epoch 5/50 - loss: 12.718417772736508 - accuracy: 11.242592592592594% - LR: 0.001\n",
      "epoch 6/50 - loss: 14.303060597107505 - accuracy: 11.242592592592594% - LR: 0.00095\n",
      "epoch 7/50 - loss: 14.306004723238194 - accuracy: 11.242592592592594% - LR: 0.00095\n",
      "epoch 8/50 - loss: 14.306004723235612 - accuracy: 11.242592592592594% - LR: 0.00095\n",
      "epoch 9/50 - loss: 14.306004723235613 - accuracy: 11.242592592592594% - LR: 0.00095\n",
      "epoch 10/50 - loss: 14.30600472323561 - accuracy: 11.242592592592594% - LR: 0.00095\n",
      "epoch 11/50 - loss: 14.306004723235604 - accuracy: 11.242592592592594% - LR: 0.0009025\n",
      "epoch 12/50 - loss: 14.306004723235606 - accuracy: 11.242592592592594% - LR: 0.0009025\n",
      "epoch 13/50 - loss: 14.30600472323562 - accuracy: 11.242592592592594% - LR: 0.0009025\n",
      "epoch 14/50 - loss: 14.306004723235615 - accuracy: 11.242592592592594% - LR: 0.0009025\n",
      "epoch 15/50 - loss: 14.306004723235612 - accuracy: 11.242592592592594% - LR: 0.0009025\n",
      "epoch 16/50 - loss: 14.306004723235604 - accuracy: 11.242592592592594% - LR: 0.000857375\n",
      "epoch 17/50 - loss: 14.30600472323561 - accuracy: 11.242592592592594% - LR: 0.000857375\n",
      "epoch 18/50 - loss: 14.30600472323561 - accuracy: 11.242592592592594% - LR: 0.000857375\n",
      "epoch 19/50 - loss: 14.30600472323561 - accuracy: 11.242592592592594% - LR: 0.000857375\n",
      "epoch 20/50 - loss: 14.306004723235613 - accuracy: 11.242592592592594% - LR: 0.000857375\n",
      "epoch 21/50 - loss: 14.30600472323561 - accuracy: 11.242592592592594% - LR: 0.0008145062499999999\n",
      "epoch 22/50 - loss: 14.306004723235613 - accuracy: 11.242592592592594% - LR: 0.0008145062499999999\n",
      "epoch 23/50 - loss: 14.30600472323561 - accuracy: 11.242592592592594% - LR: 0.0008145062499999999\n",
      "epoch 24/50 - loss: 14.306004723235612 - accuracy: 11.242592592592594% - LR: 0.0008145062499999999\n",
      "epoch 25/50 - loss: 14.30600472323561 - accuracy: 11.242592592592594% - LR: 0.0008145062499999999\n",
      "epoch 26/50 - loss: 14.306004723235612 - accuracy: 11.242592592592594% - LR: 0.0007737809374999998\n",
      "epoch 27/50 - loss: 14.306004723235604 - accuracy: 11.242592592592594% - LR: 0.0007737809374999998\n",
      "epoch 28/50 - loss: 14.30600472323561 - accuracy: 11.242592592592594% - LR: 0.0007737809374999998\n",
      "epoch 29/50 - loss: 14.306004723235606 - accuracy: 11.242592592592594% - LR: 0.0007737809374999998\n",
      "epoch 30/50 - loss: 14.306004723235613 - accuracy: 11.242592592592594% - LR: 0.0007737809374999998\n",
      "epoch 31/50 - loss: 14.306004723235604 - accuracy: 11.242592592592594% - LR: 0.0007350918906249999\n",
      "epoch 32/50 - loss: 14.306004723235615 - accuracy: 11.242592592592594% - LR: 0.0007350918906249999\n",
      "epoch 33/50 - loss: 14.306004723235606 - accuracy: 11.242592592592594% - LR: 0.0007350918906249999\n",
      "epoch 34/50 - loss: 14.306004723235615 - accuracy: 11.242592592592594% - LR: 0.0007350918906249999\n",
      "epoch 35/50 - loss: 14.306004723235606 - accuracy: 11.242592592592594% - LR: 0.0007350918906249999\n",
      "epoch 36/50 - loss: 14.306004723235617 - accuracy: 11.242592592592594% - LR: 0.0006983372960937497\n",
      "epoch 37/50 - loss: 14.306004723235606 - accuracy: 11.242592592592594% - LR: 0.0006983372960937497\n",
      "epoch 38/50 - loss: 14.306004723235606 - accuracy: 11.242592592592594% - LR: 0.0006983372960937497\n",
      "epoch 39/50 - loss: 14.306004723235613 - accuracy: 11.242592592592594% - LR: 0.0006983372960937497\n",
      "epoch 40/50 - loss: 14.306004723235615 - accuracy: 11.242592592592594% - LR: 0.0006983372960937497\n",
      "epoch 41/50 - loss: 14.306004723235612 - accuracy: 11.242592592592594% - LR: 0.0006634204312890623\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[367], line 42\u001b[0m\n\u001b[0;32m     23\u001b[0m nn \u001b[38;5;241m=\u001b[39m NeuraNetwork(\n\u001b[0;32m     24\u001b[0m     in_nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m784\u001b[39m,  \u001b[38;5;66;03m# 28*28 pixel input\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     out_nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,  \u001b[38;5;66;03m# 10 classes (digits 0-9)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     lambda_\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m\n\u001b[0;32m     39\u001b[0m )\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 42\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels_one_hot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m#loss\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[366], line 275\u001b[0m, in \u001b[0;36mNeuraNetwork.train\u001b[1;34m(self, input_data, target_data)\u001b[0m\n\u001b[0;32m    272\u001b[0m target_vector \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(batch_target[i], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    274\u001b[0m \u001b[38;5;66;03m#do the forward pass\u001b[39;00m\n\u001b[1;32m--> 275\u001b[0m activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;66;03m#calc cross-entropy loss\u001b[39;00m\n\u001b[0;32m    278\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-15\u001b[39m\n",
      "Cell \u001b[1;32mIn[366], line 158\u001b[0m, in \u001b[0;36mNeuraNetwork.forward_pass\u001b[1;34m(self, input_vector, training)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_pass\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_vector, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    156\u001b[0m \u001b[38;5;250m   \u001b[39m\u001b[38;5;124;03m\"\"\"perform a forward pass, each layer has its own weights and bias matrices as init above\"\"\"\u001b[39;00m\n\u001b[1;32m--> 158\u001b[0m    \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonitor_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    159\u001b[0m        \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight instability detected!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    161\u001b[0m    activations \u001b[38;5;241m=\u001b[39m [input_vector] \u001b[38;5;66;03m#init the activations list to be the input vector \u001b[39;00m\n",
      "Cell \u001b[1;32mIn[366], line 141\u001b[0m, in \u001b[0;36mNeuraNetwork.monitor_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Monitor weight statistics with layer-specific thresholds\"\"\"\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights):\n\u001b[1;32m--> 141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m weights contain NaN values!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "train_images = mnist_train.data.numpy().reshape(-1, 28*28) / 255.0\n",
    "train_labels = mnist_train.targets.numpy()\n",
    "test_images = mnist_test.data.numpy().reshape(-1, 28*28) / 255.0\n",
    "test_labels = mnist_test.targets.numpy()\n",
    "\n",
    "val_size = len(train_images) // 10\n",
    "val_images = train_images[:val_size]\n",
    "val_labels = train_labels[:val_size]\n",
    "train_images = train_images[val_size:]\n",
    "train_labels = train_labels[val_size:]\n",
    "\n",
    "def one_hot_encode(labels, num_classes=10):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "train_labels_one_hot = one_hot_encode(train_labels)\n",
    "val_labels_one_hot = one_hot_encode(val_labels)\n",
    "test_labels_one_hot = one_hot_encode(test_labels)\n",
    "\n",
    "#this will be my baseline\n",
    "nn = NeuraNetwork(\n",
    "    in_nodes=784,  # 28*28 pixel input\n",
    "    out_nodes=10,  # 10 classes (digits 0-9)\n",
    "    hidden_layers=[128, 64, 32],\n",
    "    hidden_activation=ReLULayer(),\n",
    "    output_activation=SoftmaxLayer(),\n",
    "    dropoutRate=0.1,\n",
    "    learning_rate=0.001,\n",
    "    optimizer='minibgd', \n",
    "    batch_size=64,\n",
    "    epochs=50,\n",
    "    decay_rate=0.95,\n",
    "    decay_steps=5,\n",
    "    min_lr=1e-6,  \n",
    "    regularization='L2',\n",
    "    lambda_=0.001\n",
    ")\n",
    "\n",
    "print(\"starting training...\")\n",
    "history = nn.train(train_images, train_labels_one_hot)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "#loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history['loss'], label='Training Loss')\n",
    "if 'val_loss' in history:\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "#accuracy\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history['accuracy'], label='Training Accuracy')\n",
    "if 'val_accuracy' in history:\n",
    "    plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy %')\n",
    "plt.legend()\n",
    "\n",
    "#learning rate\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history['learning_rate'])\n",
    "plt.title('Learning Rate Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#test set eval\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_predictions, test_metrics = nn.run(test_images, test_labels_one_hot)\n",
    "print(f\"Test Set Performance:\")\n",
    "print(f\"Loss: {test_metrics['loss']:.4f}\")\n",
    "print(f\"Accuracy: {test_metrics['accuracy']:.2f}%\")\n",
    "\n",
    "num_samples = 5\n",
    "plt.figure(figsize=(15, 3))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    plt.subplot(1, num_samples, i+1)\n",
    "    \n",
    "    #single sample prediction\n",
    "    test_sample = test_images[i]\n",
    "    predictions = nn.run(test_sample)\n",
    "    predicted_class = np.argmax(predictions)\n",
    "    true_class = np.argmax(test_labels_one_hot[i])\n",
    "    \n",
    "    plt.imshow(test_sample.reshape(28, 28), cmap='gray')\n",
    "    plt.title(f'Pred: {predicted_class}\\nTrue: {true_class}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "test_subset = test_images[:1000]\n",
    "predictions_subset = nn.run(test_subset)\n",
    "predicted_classes = np.argmax(predictions_subset, axis=1)\n",
    "true_classes = np.argmax(test_labels_one_hot[:1000], axis=1)\n",
    "\n",
    "#confusion matrix\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "#metrics\n",
    "print(\"\\nDetailed metrics for each digit:\")\n",
    "for digit in range(10):\n",
    "    mask = true_classes == digit\n",
    "    digit_acc = (predicted_classes[mask] == true_classes[mask]).mean() * 100\n",
    "    print(f\"Digit {digit} accuracy: {digit_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
