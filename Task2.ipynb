{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, I will use the CIFAR-10 dataset, which consists of 10 classes of coloured images, including airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. The dataset contains 60,000 images - 50,000 images for the training set and 10,000 images for the testing set. Each image is a 32x32 pixel RGB image (3 channels: red, green, and blue), labeled according to its class. The pixel values range from 0 to 255, representing the intensity of each color channel.\n",
    "\n",
    "Below I will load the dataset and display some of its characteristics mentioned above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 60000 images\n",
      "Test set size: 10000 images\n",
      "Image dimensions: 28x28 pixels\n",
      "Labels: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist_data = np.load('./mnist.npz')\n",
    "\n",
    "x_train, y_train = mnist_data['x_train'], mnist_data['y_train']\n",
    "x_test, y_test = mnist_data['x_test'], mnist_data['y_test']\n",
    "\n",
    "print(f\"Training set size: {x_train.shape[0]} images\")\n",
    "print(f\"Test set size: {x_test.shape[0]} images\")\n",
    "print(f\"Image dimensions: {x_train.shape[1]}x{x_train.shape[2]} pixels\")\n",
    "print(f\"Labels: {set(y_train)}\")\n",
    "\n",
    "# I will also display some sample images from the dataset to visualise the images I am working with\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(10, 2))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(x_train[i], cmap='gray')\n",
    "    ax.set_title(f\"Label: {y_train[i]}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will create a base convolutional neural network (using PyTorch) to classify these digits.\n",
    "The MNIST dataset is already almost fully pre-processed and clean, greyscaled and resized to 28x28 pixels.\n",
    "I will first normalize the pixel values into a range between 0 and 1, to help the neural network converge faster during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max pixel value: 1.0\n",
      "Min pixel value: 0.0\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# We can display the max and min value to visualise this\n",
    "\n",
    "print(f\"Max pixel value: {x_train.max()}\")\n",
    "print(f\"Min pixel value: {x_test.min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I need to convert both the training and testing set to PyTorch tensors as this is what the model expects. \n",
    "Since the neural network is convolutional, I will need to retain the 2D structure (height x width) and add an extra channel dimension for greyscale too, indicating the depth of the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32) \n",
    "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
    "\n",
    "x_train = x_train.unsqueeze(1) # becomes (60000, 1, 28, 28)\n",
    "x_test = x_test.unsqueeze(1)\n",
    "\n",
    "y_train = torch.tensor(y_train)\n",
    "y_test = torch.tensor(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are currently ranging from 0-9, so I will need to one-hot encode them into a binary vector for each, specifying the amount of classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example one-hot encoded label: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=10)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "print(f\"Example one-hot encoded label: {y_train_one_hot[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the input is prepared, I will start building the model.\n",
    "I will first need to establish a baseline CNN model, which I will build on and add improvements, and then carry out hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # 1 Convolutional Layers\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3)\n",
    "        \n",
    "        # 1 Fully Connected Layer\n",
    "        self.fc1 = nn.Linear(8 * 26 * 26, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.fc1(x)  \n",
    "        return x\n",
    "    \n",
    "model = CNN()\n",
    "\n",
    "# Baseline Hyperparameters\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, I have defined a simple base convolutional network with 1 convolutional layer and 1 fully connected layer, and no hidden layers. \n",
    "\n",
    "In the forward pass function, the first convolutional layer is applied with a ReLU activation function. Then the feature maps are flattened into 1D to be fed into the fully connected layer. Then the first fully connected layer is applied with no activation as it is handled in the loss function.\n",
    "\n",
    "Under this, I have specified the hyperparameters that I will start off with:\n",
    "- Batch size: 64 - (higher = more stable gradient updates but slower weight updates, lower = faster weight updates but gradients are noiser)\n",
    "- Learning rate: 0.01 - (higher = faster training but risk of overshooting the minimum, lower = slower training but ensures more stable and precise convergence at minimum)\n",
    "- Epochs: 3 - (higher = risk of overfitting, lower = risk of underfitting)\n",
    "\n",
    "I also need to define the loss function and the optimizer for the baseline model. I will use cross-entropy loss as it is standard for multi-class classification problems, and also the adam optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training I will also need to wrap the data in loaders now that batch size is defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will train the model for the specified baseline number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 0.2922, Accuracy: 92.57%\n",
      "Epoch [2/3], Loss: 0.0976, Accuracy: 97.20%\n",
      "Epoch [3/3], Loss: 0.0723, Accuracy: 97.71%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Print epoch statistics\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
